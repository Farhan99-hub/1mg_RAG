{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U crawl4ai"
      ],
      "metadata": {
        "id": "f2Sev3VTr2Zw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!crawl4ai-setup"
      ],
      "metadata": {
        "id": "hoSHLbowtrZk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "Ztlpuzz27X2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!crwl https://www.1mg.com/diseases/acidity-42 -o markdown #checking the crawler\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cFznnOrku7Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#main diseases page\n",
        "base_url = \"https://www.1mg.com/all-diseases\"\n",
        "\n",
        "response = requests.get(base_url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "#scraping disease names from the 1mg disease A page\n",
        "disease_links = soup.find_all(\"a\", href=True)\n",
        "\n",
        "#disease links\n",
        "disease_names = []\n",
        "for link in disease_links:\n",
        "    href = link['href']\n",
        "    if '/diseases/' in href:\n",
        "        # base url for A page:- https://www.1mg.com/diseases/\n",
        "        disease_id = href.split(\"/diseases/\")[-1]\n",
        "        disease_names.append(disease_id)\n",
        "\n",
        "\n",
        "print(disease_names[:10])\n"
      ],
      "metadata": {
        "id": "-m7y-yAYvWXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05eedd07-c052-480a-bea8-a78eaf1da080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['acidity-42', 'acne-261', 'addisons-disease-120', 'airplane-ear-970', 'allergic-conditions-10', 'alzheimers-disease-179', 'amenorrhea-467', 'anal-fissure-74', 'anaphylaxis-947', 'anorexia-nervosa-928']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "disease_names"
      ],
      "metadata": {
        "id": "bDsBEzMtxV2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "label_list = list(string.ascii_lowercase[1:]) #creating a list from B to Z to scrape disease from B to Z"
      ],
      "metadata": {
        "id": "7S-eVcevyIUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(label_list)):\n",
        "  base_url = f\"https://www.1mg.com/all-diseases?label={label_list[i]}\"  #base url for every other disease index apart from A\n",
        "\n",
        "  response = requests.get(base_url)\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "  disease_links = soup.find_all(\"a\", href=True)\n",
        "  #\n",
        "  for link in disease_links:\n",
        "    href = link['href']\n",
        "    if '/diseases/' in href:\n",
        "       #disease ID\n",
        "       disease_id = href.split(\"/diseases/\")[-1]\n",
        "       disease_names.append(disease_id)\n",
        "\n",
        "print(disease_names[:10])\n"
      ],
      "metadata": {
        "id": "RDKlD9dsztAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disease_names"
      ],
      "metadata": {
        "id": "U74GoYGvzuIl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRAWL4AI"
      ],
      "metadata": {
        "id": "3ZNTtn-ET2_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "from langchain.schema import Document\n",
        "from subprocess import run"
      ],
      "metadata": {
        "id": "d91bWhRqUj8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://www.1mg.com/diseases/\"\n",
        "output_dir = \"markdown_output\"\n",
        "\n",
        "#create directory\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for disease in disease_names:\n",
        "    url = base_url + disease\n",
        "    filename = os.path.join(output_dir, f\"{disease}.md\")\n",
        "\n",
        "    print(f\"Crawling full page for: {url}\")\n",
        "    try:\n",
        "        run(f\"crwl crawl {url} -o markdown -O {filename}\", shell=True, check=True) #crawling markdown of the disease pages\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to crawl {url}: {e}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8-SZKMdo0sMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sections_from_markdown(md_text):\n",
        "    required_sections = [\n",
        "        \"Overview\", \"Key Facts\", \"Symptoms\", \"Causes\", \"Risk factors\",\n",
        "        \"Diagnosis\", \"Prevention\",\n",
        "        \"Treatment\", \"Home-care\", \"Complications\"\n",
        "    ]\n",
        "\n",
        "    extracted = {}\n",
        "    # extracting top headers (##)\n",
        "    headers = list(re.finditer(r'^##\\s+(.*?)\\s*$', md_text, re.MULTILINE))\n",
        "\n",
        "    for i, match in enumerate(headers):\n",
        "        title = match.group(1).strip().lower()\n",
        "        start = match.end()\n",
        "        end = headers[i + 1].start() if i + 1 < len(headers) else len(md_text)\n",
        "        content = md_text[start:end].strip()\n",
        "\n",
        "        for required in required_sections:\n",
        "            if required.lower() in title:\n",
        "                extracted[required] = content\n",
        "                break\n",
        "\n",
        "    return extracted"
      ],
      "metadata": {
        "id": "ABixU7FWWTHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markdown_folder = \"markdown_output\"\n",
        "json_output_folder = \"structured_json\"\n",
        "os.makedirs(json_output_folder, exist_ok=True)\n",
        "\n",
        "for fname in os.listdir(markdown_folder):\n",
        "    if not fname.endswith(\".md\"):\n",
        "        continue\n",
        "\n",
        "    with open(os.path.join(markdown_folder, fname), \"r\", encoding=\"utf-8\") as f:  #converting markdowns to json for further processing step\n",
        "        md_content = f.read()\n",
        "\n",
        "    extracted_data = extract_sections_from_markdown(md_content)\n",
        "\n",
        "    out_path = os.path.join(json_output_folder, fname.replace(\".md\", \".json\"))\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as out_f:\n",
        "        json.dump(extracted_data, out_f, indent=2)\n",
        "\n",
        "    print(f\"Extracted: {fname} ΓåÆ {out_path}\")\n"
      ],
      "metadata": {
        "id": "mBbdJPA62lEA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For Rag"
      ],
      "metadata": {
        "id": "0zAB3_WpQVKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "\n",
        "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text) #remove markdowns\n",
        "    text = re.sub(r'[*_]{1,3}([^*_]+)[*_]{1,3}', r'\\1', text) #remove text markers\n",
        "    text = re.sub(r'^\\s*[-*]\\s*', '', text, flags=re.MULTILINE) #remove asterisks scattered to emphasise words in the markdwon file\n",
        "    text = re.sub(r'\\n','', text) #remove nextlines\n",
        "    text = re.sub(r'#','', text) #remove subheader symbols\n",
        "    text = re.sub(r'[ \\t]+', ' ', text) #remove extra space\n",
        "\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "AG1iShtQNOF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_map = {\n",
        "    \"Overview\": \"What is {disease}?\",\n",
        "    \"Key Facts\": \"What are some key facts about {disease}?\",\n",
        "    \"Symptoms\": \"What are the symptoms of {disease}?\",\n",
        "    \"Causes\": \"What causes {disease}?\",\n",
        "    \"Risk factors\": \"What are the risk factors for {disease}?\",\n",
        "    \"Diagnosis\": \"How is {disease} diagnosed?\",\n",
        "    \"Treatment\": \"How is {disease} treated?\",\n",
        "    \"Home-care\": \"What are the home remedies and care tips for {disease}?\",\n",
        "    \"Complications\": \"What complications can arise from {disease}?\",\n",
        "    \"Prevention\": \"How can {disease} be prevented?\",\n",
        "}\n",
        "\n",
        "def extract_disease_name(filename):  #removing _, -, numbers from the disease names\n",
        "    name = os.path.splitext(filename)[0]\n",
        "    name = re.sub(r'-\\d+$', '', name)\n",
        "    name = name.replace('-', ' ')\n",
        "    return name.title()\n",
        "\n",
        "def load_rag_documents(json_dir):\n",
        "    documents = []\n",
        "    for fname in os.listdir(json_dir):\n",
        "        if not fname.endswith(\".json\"):\n",
        "            continue\n",
        "        with open(os.path.join(json_dir, fname), \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        disease_name = extract_disease_name(fname)\n",
        "\n",
        "        for section, content in data.items():\n",
        "            if section in instruction_map and content.strip():\n",
        "                doc = Document(\n",
        "                    page_content=clean_text(content.strip()),\n",
        "                    metadata={\n",
        "                        \"disease\": disease_name,\n",
        "                        \"section\": section\n",
        "                    }\n",
        "                )\n",
        "                documents.append(doc)\n",
        "    return documents\n"
      ],
      "metadata": {
        "id": "AJDJWH8eL2Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = load_rag_documents(\"structured_json\")"
      ],
      "metadata": {
        "id": "lgUdknmcMClE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LPDmFRK7May5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the document as json to use it without constraint\n",
        "documents_data = [\n",
        "    {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\n",
        "    for doc in documents\n",
        "]\n",
        "\n",
        "with open(\"1mgrag_documents.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(documents_data, f, ensure_ascii=False, indent=2)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "N7Cnhh_3La_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Medicine"
      ],
      "metadata": {
        "id": "SPZwRJfv8WUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#main diseases page\n",
        "base_url = \"https://www.1mg.com/drugs-all-medicines\"\n",
        "\n",
        "response = requests.get(base_url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "#scraping disease names from the 1mg disease A page\n",
        "drug_links = soup.find_all(\"a\", href=True)\n",
        "\n",
        "#disease links\n",
        "drug_names = []\n",
        "for link in drug_links:\n",
        "    href = link['href']\n",
        "    if '/drugs/' in href:\n",
        "        # base url for A page\n",
        "        disease_id = href.split(\"/drugs/\")[-1]\n",
        "        drug_names.append(disease_id)\n",
        "\n",
        "\n",
        "print(drug_names[:10])\n"
      ],
      "metadata": {
        "id": "D2P3FlnyFD7t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0f5b42-4c3e-4c52-d758-d6a6e30855d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['avastin-100mg-injection-135666', 'actorise-40-injection-227647', 'actorise-25-injection-228329', 'actorise-60-injection-403206', 'azel-80-capsule-682932', 'avastin-400mg-injection-341311', 'azel-40mg-capsule-429353', 'actorise-200-injection-342162', 'actorise-100-injection-369472', 'azacytin-injection-341892']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "label_list = list(string.ascii_lowercase[1:]) #creating a list from B to Z to scrape disease from B to Z"
      ],
      "metadata": {
        "id": "VpSO9ingCkAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(label_list)):\n",
        "\n",
        "  #main diseases page\n",
        "  base_url = f\"https://www.1mg.com/drugs-all-medicines?label={label_list[i]}\"\n",
        "\n",
        "  response = requests.get(base_url)\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "  #scraping disease names from the 1mg disease A page\n",
        "  drug_links = soup.find_all(\"a\", href=True)\n",
        "\n",
        "  #disease links\n",
        "  for link in drug_links:\n",
        "      href = link['href']\n",
        "      if '/drugs/' in href:\n",
        "          # base url for A page\n",
        "          disease_id = href.split(\"/drugs/\")[-1]\n",
        "          drug_names.append(disease_id)\n",
        "\n",
        "\n",
        "print(drug_names[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leiTSL5FDMgN",
        "outputId": "bfde153f-4e5d-45a7-b8da-ba1e7bb31a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['avastin-100mg-injection-135666', 'actorise-40-injection-227647', 'actorise-25-injection-228329', 'actorise-60-injection-403206', 'azel-80-capsule-682932', 'avastin-400mg-injection-341311', 'azel-40mg-capsule-429353', 'actorise-200-injection-342162', 'actorise-100-injection-369472', 'azacytin-injection-341892']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(drug_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFV0NmvZ1ZJ2",
        "outputId": "7036c6e9-1d30-4f53-d99a-0ab522ec1501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "780"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "from langchain.schema import Document\n",
        "from subprocess import run"
      ],
      "metadata": {
        "id": "x8Y_fIcVEo_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = base_url = \"https://www.1mg.com/drugs/\"\n",
        "output_dir = \"markdown_output\"\n",
        "\n",
        "#create directory\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for drug in drug_names:\n",
        "    url = base_url + drug\n",
        "    filename = os.path.join(output_dir, f\"{drug}.md\")\n",
        "\n",
        "    print(f\"Crawling full page for: {url}\")\n",
        "    try:\n",
        "        run(f\"crwl crawl {url} -o markdown -O {filename}\", shell=True, check=True) #crawling markdown of the disease pages\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to crawl {url}: {e}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "g9wRsxtaETdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n"
      ],
      "metadata": {
        "id": "RU_CuhbOGnHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_drug_sections(md_text):\n",
        "    section_titles = [\n",
        "    \"Product introduction\",\n",
        "    \"Uses\",\n",
        "    \"How to use\",\n",
        "    \"How drug works\",\n",
        "    \"Side effects\",\n",
        "    \"Safety advice\",\n",
        "    \"Interaction with drugs\",\n",
        "    \"FAQs\"\n",
        "    ]\n",
        "\n",
        "    extracted = {}\n",
        "    headers = list(re.finditer(r'^##\\s+(.*?)\\s*$', md_text, re.MULTILINE))\n",
        "\n",
        "    for i, match in enumerate(headers):\n",
        "        raw_title = match.group(1).strip()\n",
        "        clean_title = raw_title.lower()\n",
        "        start = match.end()\n",
        "        end = headers[i + 1].start() if i + 1 < len(headers) else len(md_text)\n",
        "        content = md_text[start:end].strip()\n",
        "\n",
        "        for section in section_titles:\n",
        "            if section.lower() in clean_title:\n",
        "                extracted[section] = content\n",
        "                break\n",
        "\n",
        "    return extracted"
      ],
      "metadata": {
        "id": "npum4gpQu8rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)  # Remove markdown links\n",
        "    text = re.sub(r'[*_]{1,3}([^*_]+)[*_]{1,3}', r'\\1', text)  # Remove *bold*/_italic_\n",
        "    text = re.sub(r'^[-*]\\s+', '', text, flags=re.MULTILINE)  # List bullets\n",
        "    text = re.sub(r'\\n+', ' ', text)  # Collapse newlines\n",
        "    text = re.sub(r'#', '', text)  # Remove any stray headers\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)  # Remove extra whitespace\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "gLxEzK0bvMGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markdown_folder = \"markdown_output\"\n",
        "json_output_folder = \"drug_json\"\n",
        "os.makedirs(json_output_folder, exist_ok=True)\n",
        "\n",
        "for fname in os.listdir(markdown_folder):\n",
        "    if not fname.endswith(\".md\"):\n",
        "        continue\n",
        "\n",
        "    with open(os.path.join(markdown_folder, fname), \"r\", encoding=\"utf-8\") as f:\n",
        "        md_content = f.read()\n",
        "\n",
        "    extracted = extract_drug_sections(md_content)\n",
        "    cleaned = {k: clean_text(v) for k, v in extracted.items()}\n",
        "\n",
        "    out_path = os.path.join(json_output_folder, fname.replace(\".md\", \".json\"))\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as out_f:\n",
        "        json.dump(cleaned, out_f, indent=2)\n",
        "\n",
        "    print(f\"Extracted: {fname} ΓåÆ {out_path}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "709QJmpyvdih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_drug_sections(md_text):\n",
        "    section_titles = [\n",
        "        \"Product introduction\", \"Uses\", \"How to use\", \"How drug works\",\n",
        "        \"Side effects\", \"Safety advice\", \"Interaction with drugs\", \"FAQs\"\n",
        "    ]\n",
        "\n",
        "    extracted = []\n",
        "    headers = list(re.finditer(r'^##\\s+(.*?)\\s*$', md_text, re.MULTILINE))\n",
        "\n",
        "    for i, match in enumerate(headers):\n",
        "        title = match.group(1).strip()\n",
        "        title_clean = title.lower()\n",
        "        start = match.end()\n",
        "        end = headers[i + 1].start() if i + 1 < len(headers) else len(md_text)\n",
        "        content = md_text[start:end].strip()\n",
        "\n",
        "        for section in section_titles:\n",
        "            if section.lower() in title_clean:\n",
        "                extracted.append((section, content))\n",
        "                break\n",
        "\n",
        "    return extracted\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)\n",
        "    text = re.sub(r'[*_]{1,3}([^*_]+)[*_]{1,3}', r'\\1', text)\n",
        "    text = re.sub(r'^[-*]\\s+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r'#', '', text)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "wbZIOe_fvf5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markdown_folder = \"markdown_output\"\n",
        "output_path = \"drug_rag_documents.json\"\n",
        "\n",
        "rag_entries = []\n",
        "\n",
        "for fname in os.listdir(markdown_folder):\n",
        "    if not fname.endswith(\".md\"):\n",
        "        continue\n",
        "\n",
        "    with open(os.path.join(markdown_folder, fname), \"r\", encoding=\"utf-8\") as f:\n",
        "        md_content = f.read()\n",
        "\n",
        "    drug_name = fname.replace(\".md\", \"\").replace(\"-\", \" \").title()\n",
        "    sections = extract_drug_sections(md_content)\n",
        "\n",
        "    for section_title, raw_content in sections:\n",
        "        cleaned_content = clean_text(raw_content)\n",
        "        rag_entries.append({\n",
        "            \"page_content\": cleaned_content,\n",
        "            \"metadata\": {\n",
        "                \"drug\": drug_name,\n",
        "                \"section\": section_title\n",
        "            }\n",
        "        })\n",
        "\n",
        "# Save as single JSON file\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
        "    json.dump(rag_entries, out_f, indent=2)\n",
        "\n",
        "print(f\"Generated {len(rag_entries)} drug RAG entries ΓåÆ {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQaPQbRExb4Z",
        "outputId": "18f01474-c069-458d-c2e3-a9ca6ee3dec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 4951 drug RAG entries ΓåÆ drug_rag_documents.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = \"combined_rag_documents.jsonl\"\n",
        "file1 = \"1mgrag_documents.json\"\n",
        "file2 = \"drug_rag_documents.json\"\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    # Read and write data from file1\n",
        "    with open(file1, 'r', encoding='utf-8') as infile1:\n",
        "        data1 = json.load(infile1)\n",
        "        for entry in data1:\n",
        "            json.dump(entry, outfile, ensure_ascii=False)\n",
        "            outfile.write('\\n')\n",
        "\n",
        "    # Read and write data from file2\n",
        "    with open(file2, 'r', encoding='utf-8') as infile2:\n",
        "        data2 = json.load(infile2)\n",
        "        for entry in data2:\n",
        "            json.dump(entry, outfile, ensure_ascii=False)\n",
        "            outfile.write('\\n')\n",
        "\n",
        "print(f\"Combined {file1} and {file2} into {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMNJpgFRxeuE",
        "outputId": "add1e47e-3672-4eeb-b78f-5ea6c07f7f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined 1mgrag_documents.json and drug_rag_documents.json into combined_rag_documents.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "17dvl7_H1x9M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
